<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="人工智能作业220049200019 凌磊 下图所示为一个简单的多层感知器(多层前馈神经网络)：包含输入层，一个隐含层，输出层， 具体如下。  输入层: 2个神经  隐含层: 3个神经元,，激活函数为 sigmoid函数  输出层: 1个神经元,，激活函数为 sigmoid函数   (1) 自行随机设定参数(取值[0，1])进行参数初始化；针对样本(x，y)，其中x&#x3D;[0.5 0.8]^">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2024/05/04/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BD%9C%E4%B8%9A2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="人工智能作业220049200019 凌磊 下图所示为一个简单的多层感知器(多层前馈神经网络)：包含输入层，一个隐含层，输出层， 具体如下。  输入层: 2个神经  隐含层: 3个神经元,，激活函数为 sigmoid函数  输出层: 1个神经元,，激活函数为 sigmoid函数   (1) 自行随机设定参数(取值[0，1])进行参数初始化；针对样本(x，y)，其中x&#x3D;[0.5 0.8]^">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://image.xiaobeo.top/img/202311301342225.png">
<meta property="article:published_time" content="2024-05-04T05:28:05.114Z">
<meta property="article:modified_time" content="2024-05-04T04:44:43.143Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://image.xiaobeo.top/img/202311301342225.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-人工智能作业2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/04/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BD%9C%E4%B8%9A2/" class="article-date">
  <time class="dt-published" datetime="2024-05-04T05:28:05.114Z" itemprop="datePublished">2024-05-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="人工智能作业2"><a href="#人工智能作业2" class="headerlink" title="人工智能作业2"></a>人工智能作业2</h1><p>20049200019 凌磊</p>
<p>下图所示为一个简单的多层感知器(多层前馈神经网络)：包含输入层，一个隐含层，输出层， 具体如下。</p>
<ul>
<li><p>输入层: 2个神经</p>
</li>
<li><p>隐含层: 3个神经元,，激活函数为 sigmoid函数</p>
</li>
<li><p>输出层: 1个神经元,，激活函数为 sigmoid函数</p>
<img src="https://image.xiaobeo.top/img/202311301342225.png" alt="image-20231130134246167" style="zoom:50%;" /></li>
</ul>
<p>(1) 自行随机设定参数(取值[0，1])进行参数初始化；针对样本(x，y)，其中x&#x3D;[0.5 0.8]^T^，y&#x3D;1，试利用前向传播计算每一层神经元的激活值a^(l)^，l&#x3D;1，2，3。并计算目标函数—损失(采用交叉熵损失函数)。</p>
<p>(2) 利用反向传播计算每一层的误差δ^(l)^，l &#x3D; 2，3，并计算梯度$\frac{\partial J(\Theta)}{\partial \Theta^{(l)}}$<br>，l&#x3D;1,2。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_derivative</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> sigmoid(z) * (<span class="number">1</span> - sigmoid(z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">z</span>):</span><br><span class="line">    exp_z = np.exp(z - np.<span class="built_in">max</span>(z))  <span class="comment"># 避免数值稳定性问题</span></span><br><span class="line">    <span class="keyword">return</span> exp_z / exp_z.<span class="built_in">sum</span>(axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自行随机设定参数(取值[0，1])进行参数初始化</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">W1 = np.random.rand(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b1 = np.random.rand(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">W2 = np.random.rand(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">b2 = np.random.rand(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入样本</span></span><br><span class="line">x = np.array([[<span class="number">0.5</span>], [<span class="number">0.8</span>]])</span><br><span class="line">y = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">a1 = x</span><br><span class="line">z2 = np.dot(W1, a1) + b1</span><br><span class="line">a2 = sigmoid(z2)</span><br><span class="line">z3 = np.dot(W2, a2) + b2</span><br><span class="line">a3 = sigmoid(z3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">J = -y * np.log(a3) - (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - a3)</span><br><span class="line">loss = np.<span class="built_in">sum</span>(J)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">delta3 = a3 - y <span class="comment">#输出层的误差</span></span><br><span class="line">delta2 = np.dot(W2.T, delta3) * sigmoid_derivative(z2) <span class="comment">#隐含层的误差</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度输出层权重的梯度为：dW2</span></span><br><span class="line">dW2 = np.dot(delta3, a2.T)</span><br><span class="line"><span class="comment">#输出层偏置的梯度为：db2</span></span><br><span class="line">db2 = np.<span class="built_in">sum</span>(delta3, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#隐含层权重的梯度为：dW1</span></span><br><span class="line">dW1 = np.dot(delta2, a1.T)</span><br><span class="line"><span class="comment">#隐含层偏置的梯度为：db1</span></span><br><span class="line">db1 = np.<span class="built_in">sum</span>(delta2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入层到隐含层的权重矩阵为W1&quot;</span>,W1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入层到隐含层的偏置为b1=&quot;</span>,b1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;隐含层到输出层的权重矩阵为W2=&quot;</span>,W2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;隐含层到输出层的偏置为b2=&quot;</span>,b2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;前向传播结果:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a1 =&quot;</span>, a1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a2 =&quot;</span>, a2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a3 =&quot;</span>, a3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;损失:&quot;</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n反向传播结果:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;delta3 =&quot;</span>, delta3)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;delta2 =&quot;</span>, delta2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度 dW2 =&quot;</span>, dW2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度 db2 =&quot;</span>, db2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度 dW1 =&quot;</span>, dW1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度 db1 =&quot;</span>, db1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>下面为一次随机参数输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">输入层到隐含层的权重矩阵为W1=</span><br><span class="line">[[4.17022005e-01 7.20324493e-01]</span><br><span class="line">[1.14374817e-04 3.02332573e-01]</span><br><span class="line">[1.46755891e-01 9.23385948e-02]]</span><br><span class="line">输入层到隐含层的偏置为b1=</span><br><span class="line">[[0.18626021]</span><br><span class="line">[0.34556073]</span><br><span class="line">[0.39676747]]</span><br><span class="line">隐含层到输出层的权重矩阵为W2=</span><br><span class="line">[[0.53881673 0.41919451 0.6852195 ]]</span><br><span class="line">隐含层到输出层的偏置为b2=</span><br><span class="line">[[0.20445225]]</span><br><span class="line">前向传播结果:</span><br><span class="line">a1 = [[0.5]</span><br><span class="line">[0.8]]</span><br><span class="line">a2 = [[0.72532491]</span><br><span class="line">[0.64278764]</span><br><span class="line">[0.63274621]]</span><br><span class="line">a3 = [[0.78554748]]</span><br><span class="line">损失: 0.24137437540286194</span><br><span class="line"></span><br><span class="line">反向传播结果:</span><br><span class="line">delta3 = [[-0.21445252]]</span><br><span class="line">delta2 = [[-0.023021  ]</span><br><span class="line">[-0.02064148]</span><br><span class="line">[-0.03414733]]</span><br><span class="line">梯度 dW2 = [[-0.15554775 -0.13784743 -0.13569402]]</span><br><span class="line">梯度 db2 = [[-0.21445252]]</span><br><span class="line">梯度 dW1 = [[-0.0115105  -0.0184168 ]</span><br><span class="line">[-0.01032074 -0.01651318]</span><br><span class="line">[-0.01707366 -0.02731786]]</span><br><span class="line">梯度 db1 = [[-0.023021  ]</span><br><span class="line">[-0.02064148]</span><br><span class="line">[-0.03414733]]</span><br></pre></td></tr></table></figure>

<p>(3) 如果采用多层感知器进行多分类，假设类别总数C&#x3D;5，试问输出层神经元个数应设置为多少个？输出层每个神经元的激活值是什么含义?</p>
<blockquote>
<p> 输出层神经元个数应设置为 5。每个输出层神经元的激活值表示对应类别的预测概率。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/05/04/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%BD%9C%E4%B8%9A2/" data-id="clvrpglqo0004ncq7g0zxa5a0" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/05/04/%E5%8F%8B%E9%93%BE/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2024/05/04/WEIJI/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/test/">test</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E4%B8%89/">大三</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%B0%E5%BD%95/">记录</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%83%E4%B9%A0/" rel="tag">练习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%B0%E5%BD%95/" rel="tag">记录</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">笔记</a> <a href="/tags/%E7%BB%83%E4%B9%A0/" style="font-size: 10px;">练习</a> <a href="/tags/%E8%AE%B0%E5%BD%95/" style="font-size: 20px;">记录</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/05/04/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%9F%BA%E7%A1%80%E4%B8%8ECAD%E5%BC%80%E5%8F%91/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/05/04/%E8%A5%BF%E7%94%B5%E6%A0%A1%E5%9B%AD%E7%BD%91%E8%BF%9B%E9%98%B6%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/05/04/%E7%94%B5%E8%A1%A8%E6%AC%A2%E8%BF%8E2023%E7%BA%A7%E6%96%B0%E7%94%9F%E7%9A%84%E5%88%B0%E6%9D%A5/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/05/04/%E7%AE%B1%E4%BD%93%E8%A3%85%E9%85%8D%E5%9B%BE%E6%98%8E%E7%BB%86%E6%A0%8F/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/05/04/%E7%94%B5%E8%A1%A8%E6%AC%A2%E8%BF%8E2023%E5%B1%8A%E6%96%B0%E7%94%9F%E7%9A%84%E5%88%B0%E6%9D%A5/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>